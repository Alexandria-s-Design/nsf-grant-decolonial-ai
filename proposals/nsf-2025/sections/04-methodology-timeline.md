# Research Methodology and Project Timeline

## 1. Research Design Overview

### 1.1 Paradigmatic Foundation

This research is grounded in **critical participatory paradigms** (Freire, 1970; Cammarota & Fine, 2008), specifically:

- **Participatory Action Research (PAR)**: Communities as co-researchers driving all phases of inquiry
- **Critical Race Theory (CRT)**: Centering race and racism in analysis of educational systems (Ladson-Billings & Tate, 1995; Solórzano & Yosso, 2002)
- **Indigenous Research Methodologies**: Relationality, reciprocity, community sovereignty (Kovach, 2009; Wilson, 2008; Smith, 2012)
- **Decolonial Praxis**: Refusal of extractive research; knowledge production for community liberation (Tuck & Yang, 2012)

### 1.2 Mixed-Methods Convergent Design

**Rationale**: Complex research questions require both depth (qualitative) and breadth (quantitative) to understand *how* and *why* decolonial AI pedagogy works, *for whom*, and *under what conditions*.

**Design**: QUAL + QUAN → Integration (Creswell & Plano Clark, 2018)
- Qualitative and quantitative data collected concurrently
- Equal priority to both strands
- Integration through joint displays, meta-inferences, and participatory interpretation

**Validity Strategies**:
- Triangulation across data sources
- Member checking with Community Research Teams
- Prolonged engagement (3-year study)
- Researcher reflexivity journals
- External audit by decolonial scholars

## 2. Research Sites and Participants

### 2.1 Partner Site Selection (Year 1, Months 1-3)

**Purposeful Sampling**: 30 partner schools across 15 districts selected for maximum variation (Patton, 2015):

**Geographic Diversity**:
- West (8 schools): CA, WA, AZ, NM
- Midwest (6 schools): MN, WI, IL
- South (8 schools): TX, MS, NC, FL
- Northeast (6 schools): NY, MA, ME
- Territories (2 schools): Puerto Rico, Navajo Nation

**Community Type**:
- Urban (12 schools): Cities >100,000 population
- Suburban (6 schools): Adjacent to urban centers
- Rural (8 schools): <20,000 population
- Tribal (4 schools): Serving Indigenous communities on/near reservations

**Student Demographics** (target across all sites):
- 65%+ students of color
- 45%+ economically disadvantaged
- 20%+ English Language Learners
- 15%+ students with disabilities

**Institutional Readiness**:
- Administrative commitment (documented through MOU)
- Existing community partnerships
- Willingness to share data and decision-making power
- Capacity for participatory research (release time, meeting space)

**Selection Process**:
1. National call for applications (via CSTA, ISTE, state education agencies)
2. Review by Community Selection Committee (10 educators, community members from pilot sites)
3. Site visits to finalists (assess readiness, build relationships)
4. Mutual agreement on expectations (MOUs signed)

### 2.2 Participant Groups

**Educators** (n = 500):
- **Lead Teachers** (n = 200): Implementing decolonial AI curriculum
- **Supporting Teachers** (n = 200): Science, math, social studies teachers integrating AI
- **Instructional Coaches** (n = 50): Supporting implementation
- **Administrators** (n = 50): Principals, curriculum directors

**Students** (n = 10,000):
- Grades 6-12 participating in decolonial AI courses/units
- Diverse demographics (see 2.1 above)
- Youth Participatory Action Research (YPAR) teams (n = 300 student co-researchers)

**Community Members** (n = 300):
- Families (n = 150): Parents/guardians of participating students
- Tribal Elders (n = 30): Cultural knowledge keepers in Indigenous communities
- Community Organizers (n = 50): Local leaders, non-profit directors
- Community Research Team members (n = 70): Paid co-researchers

**Comparison Group** (quasi-experimental):
- 15 matched schools (propensity score matching on demographics, prior CS enrollment)
- Business-as-usual AI/CS instruction
- Same outcome measures as intervention schools

## 3. Data Collection Methods

### 3.1 Qualitative Data Collection

**Method 1: Ethnographic Classroom Observation** (1,200+ hours total)

**Purpose**: Document decolonial AI pedagogical practices in naturalistic settings

**Protocol**:
- Each partner classroom observed 8 times per year (3 years) = 40 hours per site × 30 sites = 1,200 hours
- Structured observation protocol: The Right Path Pedagogical Rubric (4 dimensions × 10 indicators)
- Field notes: Thick description of interactions, student engagement, cultural practices
- Video recording (with consent): Capture complex interactions for microanalysis
- Artifacts: Student work samples, curriculum materials, classroom displays

**Analysis**:
- First cycle coding: Descriptive codes aligned to framework dimensions
- Second cycle coding: Pattern codes identifying pedagogical moves, student responses, cultural practices
- Cross-case analysis: Identify common patterns and contextual variations
- Video microanalysis: Interaction analysis of critical moments (e.g., students contesting bias in AI system)

**Method 2: Semi-Structured Interviews** (n = 150 interviews × 60 min = 150 hours)

**Participant Types**:
- Lead teachers (n = 60): Pedagogical decision-making, challenges, growth
- Students (n = 40): Learning experiences, identity development, future aspirations
- Administrators (n = 20): Policy changes, institutional barriers/supports
- Community members (n = 30): Partnership experiences, community benefits

**Protocol Development**: Co-created with Community Research Teams; piloted and refined

**Key Topics**:
- Experiences with decolonial AI pedagogy
- Changes in beliefs, knowledge, practices
- Barriers and facilitators to implementation
- Cultural relevance and community connection
- Recommendations for improvement

**Analysis**: Thematic analysis (Braun & Clarke, 2006) using decolonial frameworks; member checking with interviewees

**Method 3: Focus Groups** (n = 40 groups × 90 min)

**Composition**:
- Student focus groups (n = 20): 6-8 students per group, grade-level cohorts
- Educator focus groups (n = 10): 8-10 teachers discussing implementation
- Mixed stakeholder groups (n = 10): Students, teachers, families, community members

**Topics**: Implementation successes/challenges, cultural responsiveness, community impact, scalability

**Facilitation**: Trained facilitators from Community Research Teams; culturally responsive protocols

**Analysis**: Collaborative analysis sessions with CRT members; thematic coding

**Method 4: Document and Artifact Analysis**

**Document Types**:
- Curriculum materials (lesson plans, assessments, resources)
- Student work (AI projects, reflections, portfolios)
- Policy documents (district AI policies, data governance, vendor contracts)
- Meeting minutes (Community Advisory Boards, professional learning communities)

**Analysis**: Critical discourse analysis (Gee, 2014) examining how language constructs students, communities, and AI

**Method 5: Youth Participatory Action Research (YPAR)** (n = 300 student researchers)

**Structure**: 10 students per partner site form YPAR teams

**Process** (adapted from Cammarota & Fine, 2008):
1. **Critical Inquiry**: Students identify research questions about AI in their schools (e.g., "Does facial recognition work equally for all students?", "Who benefits from adaptive learning software?")
2. **Data Collection**: Students design and implement research (surveys, interviews, observations)
3. **Analysis**: Students analyze data using critical frameworks
4. **Action**: Students present findings to administrators, propose policy changes

**Project Support**: $2,000 per YPAR team for materials, stipends; adult facilitator training

**Research Role**: YPAR findings integrated into project data; students co-author conference presentations and articles

### 3.2 Quantitative Data Collection

**Instrument 1: AI & Human Flourishing Readiness Index** (Pre/Post, Years 1-3)

**Development Process** (Year 1):
1. Item generation based on framework dimensions (100 initial items)
2. Expert review (10 decolonial scholars, 10 educators) → 60 items
3. Cognitive interviews with students/teachers (n = 30) → refinement
4. Pilot testing (n = 500 students, n = 100 teachers at non-partner schools)
5. Psychometric analysis: Factor analysis, reliability, differential item functioning

**Versions**:
- **Student Version**: 40 items across 4 subscales (epistemological pluralism, data sovereignty, algorithmic literacy, human flourishing)
- **Educator Version**: 50 items across 4 subscales (pedagogical practices, institutional policies, community partnerships, personal growth)
- **Institutional Audit**: 30 items (policies, structures, resource allocation)

**Administration**: Online survey, 20-30 minutes; administered at beginning and end of each school year

**Sample Items** (Student Version):
- "In my AI class, we learn about how Indigenous peoples use computational thinking" (Epistemological Pluralism)
- "I understand who controls the data collected about me at school" (Data Sovereignty)
- "I can identify bias in AI systems" (Algorithmic Literacy)
- "AI projects in my class help my community" (Human Flourishing)

**Psychometric Targets**: α > .80 for each subscale; CFA fit indices (CFI > .95, RMSEA < .06)

**Instrument 2: CS Identity & Belonging Scale** (Adapted from Charleston et al., 2014)

**Subscales**:
- CS Identity: "I see myself as a computer scientist"
- Sense of Belonging: "I feel welcomed in CS spaces"
- STEM Self-Efficacy: "I am confident I can succeed in CS"

**Administration**: Pre/post each year; 15 items, 10 minutes

**Instrument 3: Learning Analytics** (Continuous data collection)

**Data Sources**:
- Course enrollment and completion rates (disaggregated by demographics)
- Project completion and quality (rubric-scored)
- Skill assessments (computational thinking, AI concepts)
- Engagement metrics (attendance, participation, portfolio artifacts)

**Privacy**: All data de-identified; IRB-approved protocols; FERPA compliance; tribal data sovereignty agreements

**Instrument 4: Broadening Participation Metrics**

**National Comparison Data**: Code.org State of CS Education reports, College Board AP CS data

**Disaggregation**: All metrics reported by race/ethnicity, gender, socioeconomic status, disability status, English Language Learner status, geographic location

**Metrics**:
- CS course enrollment rates
- Advanced CS course participation (AP, dual enrollment)
- Persistence in CS (year-over-year retention)
- Post-secondary CS major declarations (alumni tracking with consent)

### 3.3 Participatory Data Collection by Community Research Teams

**CRT Structure**: 30 teams (one per site), 5-7 members each (210 total community co-researchers)

**Compensation**: $5,000 annual team stipend + $100 per meeting per member

**Training** (Year 1, 40 hours):
- Participatory research methods
- Qualitative data collection (interviews, observations, focus groups)
- Quantitative data literacy
- Ethical research protocols
- Data sovereignty and community rights

**CRT Research Activities**:
- Co-design all research protocols
- Lead focus groups and community interviews
- Conduct cultural relevance assessments of curriculum
- Analyze data through community lenses
- Determine dissemination strategies
- Control data use and governance

**Data Sovereignty**: Each CRT establishes data governance protocols determining:
- What data can be collected
- Who has access
- How data is analyzed and shared
- Community ownership and benefit

## 4. Data Analysis

### 4.1 Qualitative Analysis

**Software**: NVivo 14 for coding and analysis; Dedoose for collaborative analysis with CRTs

**Coding Process**:

**Phase 1: Deductive Coding** (Framework-driven)
- A priori codes aligned to The Right Path dimensions
- Deductive codebook developed from literature and framework
- Reliability: Dual coding 20% of data, Cohen's κ > .80

**Phase 2: Inductive Coding** (Data-driven)
- Open coding identifying emergent themes
- In vivo codes capturing participant language
- Constant comparative method (Glaser & Strauss, 1967)

**Phase 3: Pattern Coding**
- Grouping codes into themes
- Cross-case analysis identifying variations
- Critical incident analysis

**Participatory Analysis**: Quarterly data interpretation sessions with CRTs analyzing findings from their sites; community knowledge integrated into interpretations

**Trustworthiness Strategies** (Lincoln & Guba, 1985):
- Credibility: Prolonged engagement, triangulation, member checking
- Transferability: Thick description, maximum variation sampling
- Dependability: Audit trail, reflexivity journals
- Confirmability: Data repositories, external audit

### 4.2 Quantitative Analysis

**Software**: R with lavaan (SEM), lme4 (HLM), MatchIt (propensity scores)

**Analysis 1: Psychometric Validation** (Year 1)
- **Exploratory Factor Analysis**: Determine factor structure of Index
- **Confirmatory Factor Analysis**: Validate hypothesized 4-factor model
- **Reliability**: Cronbach's α, McDonald's ω for each subscale
- **Differential Item Functioning**: Ensure items function equivalently across demographic groups

**Analysis 2: Descriptive Statistics**
- Means, standard deviations, frequencies for all variables
- Disaggregation by demographic groups
- Growth trajectories over 3 years

**Analysis 3: Inferential Statistics**

**Hierarchical Linear Modeling (HLM)**: Account for nesting (students within classrooms within schools)

Model specification:
- **Level 1** (Student): Outcome ~ Time + Demographics + (random effects)
- **Level 2** (Classroom): Teacher characteristics, pedagogical practices
- **Level 3** (School): Institutional policies, community context

**Quasi-Experimental Comparison**: Propensity score matching
- Match intervention and comparison schools on demographics, prior CS enrollment, geographic context
- Compare outcomes using matched pairs t-tests or ANCOVA
- Sensitivity analyses for hidden bias

**Moderation Analyses**: Examine differential effects by:
- Student demographics (race, SES, disability, ELL status)
- School context (urban/rural, tribal, resource level)
- Implementation fidelity

**Effect Sizes**: Report Cohen's d for all comparisons; interpret using What Works Clearinghouse standards

### 4.3 Mixed-Methods Integration

**Integration Strategy 1: Convergence** (Fetters, Curry, & Creswell, 2013)
- Joint displays comparing qualitative themes with quantitative results
- Meta-inferences: Do QUAL and QUAN findings converge or diverge?
- Explanation: Use qualitative data to explain quantitative patterns

**Integration Strategy 2: Expansion**
- Qualitative data illuminates *how* and *why* behind quantitative outcomes
- Quantitative data identifies patterns for deeper qualitative exploration

**Integration Strategy 3: Participatory Interpretation**
- CRTs interpret integrated findings
- Community knowledge explains statistical patterns
- Quantitative data validates community observations

**Example Joint Display**:

| Framework Dimension | Quantitative Finding | Qualitative Theme | Integration |
|---------------------|----------------------|-------------------|-------------|
| Epistemological Pluralism | 85% students agree "multiple ways of knowing valued" | Students describe learning Indigenous math, African innovations | Convergence: Both confirm cultural integration |
| Data Sovereignty | 60% students understand data rights | Teachers struggle to explain school data practices | Divergence: Student knowledge exceeds teacher implementation |

## 5. Project Timeline (36 Months)

### Year 1: Foundation & Development (Months 1-12)

**Months 1-3: Project Launch & Site Selection**
- Assemble research team, hire staff
- National call for partner schools
- Site selection process (applications, reviews, visits)
- MOUs signed with 30 partner schools
- Form Community Research Teams at each site
- IRB approvals, tribal research agreements

**Months 4-6: Instrument Development & CRT Training**
- Develop AI & Human Flourishing Readiness Index (item generation, expert review)
- Pilot test Index (n = 500 students, 100 teachers at non-partner sites)
- Psychometric analysis and refinement
- Develop observation protocols, interview guides
- CRT Training Week 1: Participatory research foundations (40 hours)
- Establish data governance protocols with each CRT

**Months 7-9: Framework Refinement & Curriculum Development**
- Literature synthesis workshops with CRTs
- Participatory framework refinement (community input on The Right Path dimensions)
- Curriculum writing sprint: Develop 30 initial lessons (6 per grade level 6-10)
- Pilot lessons at 5 sites
- Educator Professional Development Tier 1 begins (asynchronous course)

**Months 10-12: Baseline Data Collection & PD Launch**
- Administer pre-assessments (Index, CS Identity Scale) at all sites
- Baseline interviews with teachers (n = 60)
- Baseline focus groups with students (n = 20)
- Document analysis: Existing policies, curricula
- Educator PD Tier 2: Summer Institute (5 days, 200 teachers)
- Year 1 annual convening: All CRTs, research team
- YPAR teams formed at each site

**Year 1 Deliverables**:
- Validated AI & Human Flourishing Readiness Index
- Refined The Right Path Framework (Version 2.0)
- 30 pilot-tested lessons
- 500 educators completing Tier 1 PD
- Baseline data collected
- Year 1 Progress Report to NSF

### Year 2: Implementation & Iteration (Months 13-24)

**Months 13-15: Full Implementation Launch**
- All 30 sites implementing decolonial AI curriculum
- Monthly classroom observations begin (2 per site per month)
- YPAR teams conducting local research
- Educator PLCs meeting bi-weekly
- Community Advisory Boards meeting monthly
- Technical assistance provided by research team

**Months 16-18: Mid-Year Data Collection & Formative Evaluation**
- Mid-year Index administration
- Teacher interviews (n = 40)
- Student focus groups (n = 20)
- CRT-led community focus groups
- Implementation fidelity assessments
- Formative evaluation: Identify needed adjustments
- Curriculum refinement based on feedback

**Months 19-21: Expansion & Deepening**
- Develop advanced curriculum (20 additional lessons)
- Educator PD Tier 3 begins: Leadership cohort (50 teachers)
- YPAR teams present findings to school boards
- Policy advocacy training for CRTs
- Begin vendor audit protocol development
- Student showcase events at each site

**Months 22-24: End-of-Year 2 Data Collection**
- Post-Year 2 Index administration
- Teacher interviews (n = 60)
- Student interviews (n = 40)
- Administrator interviews (n = 20)
- Document analysis: Policy changes, curriculum artifacts
- Comparison school data collection (matched schools)
- Year 2 annual convening
- Preliminary data analysis and interpretation sessions with CRTs

**Year 2 Deliverables**:
- Complete curriculum package (50+ lessons, grades 6-12)
- Practitioner toolkit (draft)
- 2 peer-reviewed publications submitted
- 5 conference presentations
- Year 2 Progress Report to NSF

### Year 3: Scaling, Evaluation, & Dissemination (Months 25-36)

**Months 25-27: Continued Implementation & Scaling Preparation**
- Ongoing implementation at 30 partner sites
- Train-the-trainer: 50 teacher-leaders certified as PD facilitators
- Vendor audit protocol finalized and piloted
- National webinar series launched (quarterly, ongoing)
- Book manuscript drafting begins
- Replication sites recruited (10 non-partner schools piloting curriculum)

**Months 28-30: Comprehensive Evaluation**
- Final Index administration (Year 3 post-test)
- Longitudinal data analysis (3-year trajectories)
- Teacher interviews (n = 60)
- Student interviews (n = 40)
- Community impact assessments (CRT-led)
- Administrator interviews (n = 30)
- Comparison school Year 3 data collection
- HLM analyses, quasi-experimental comparisons
- Integrated QUAL-QUAN analysis

**Months 31-33: Synthesis & Dissemination**
- Participatory data interpretation: 3-day retreat with all CRTs
- Meta-inferences and integrated findings
- Policy brief development (federal, state, district levels)
- Academic publications: 5 manuscripts submitted
- Practitioner toolkit finalized (open-source release)
- Documentary film production
- Conference presentations (AERA, NSTA, SIGCSE, ISTE)

**Months 34-36: Sustainability Planning & Closeout**
- Learning network transition to community governance
- Regional hub establishment (10 hubs)
- Follow-on funding proposals submitted
- Final NSF report preparation
- Book publication (*The Right Path: A Practical Guide*)
- National dissemination campaign
- Project archive and data repository (with CRT data sovereignty protocols)
- Sustainability planning with partner schools
- Celebration events at each site

**Year 3 Deliverables**:
- Complete practitioner toolkit (open-source)
- Book publication
- 10 peer-reviewed publications submitted
- Policy briefs (federal, state, district)
- Documentary film
- Final NSF report
- Sustainability plan

## 6. Project Management and Coordination

### 6.1 Organizational Structure

**Principal Investigator (PI)**: Dr. Charles Martin
- Overall project leadership, research direction
- Liaison with NSF
- Budget management and reporting

**Co-Principal Investigators** (to be recruited):
- **Co-PI 1**: Decolonial education scholar
- **Co-PI 2**: Computer science education researcher
- **Co-PI 3**: Indigenous education expert

**Senior Personnel**:
- **Project Director** (1.0 FTE): Day-to-day management, site coordination
- **Research Director** (0.5 FTE): Data collection/analysis oversight
- **Curriculum Director** (0.5 FTE): Curriculum development and PD
- **Community Liaison** (0.5 FTE): CRT support and coordination

**Research Team**:
- Graduate Research Assistants (6 @ 0.5 FTE each): Data collection, analysis
- Community Coordinators (15 @ 0.25 FTE each): Site-level support
- Data Analyst (0.5 FTE): Quantitative analysis

**Advisory Board** (meets bi-annually):
- 5 decolonial/Indigenous scholars
- 5 K-12 educators from non-partner schools
- 5 community organizers
- 3 policy experts
- 2 industry representatives (ethical AI companies)

### 6.2 Communication and Coordination

**Monthly**: PI team meetings (PI, Co-PIs, senior personnel)
**Bi-weekly**: Site coordinators meetings (virtual)
**Quarterly**: All-team meetings (research team + CRT representatives)
**Bi-annually**: Advisory Board meetings
**Annually**: Full convening (all CRTs, research team, Advisory Board)

**Technology Platforms**:
- Slack: Daily communication
- Zoom: Virtual meetings
- Google Drive: Document sharing (with CRT access controls)
- GitHub: Curriculum version control
- Notion: Project management and task tracking

### 6.3 Risk Management

**Potential Risk**: Low partner school recruitment
- **Mitigation**: Early national outreach; partner with professional organizations; competitive stipends

**Potential Risk**: High teacher/site attrition
- **Mitigation**: Robust support structures; flexible implementation; incentives for continuation

**Potential Risk**: Data collection challenges (IRB, tribal agreements)
- **Mitigation**: Early IRB submissions; culturally responsive protocols; build in extra time

**Potential Risk**: Fidelity variation across sites
- **Mitigation**: Implementation rubrics; formative evaluation; adaptive support

**Potential Risk**: COVID-19 or other disruptions
- **Mitigation**: Hybrid/virtual options for PD and data collection; flexible timeline adjustments

### 6.4 Ethical Considerations

**Human Subjects Protection**:
- IRB approval from PI institution
- Site-specific IRB approvals as needed
- Tribal research agreements (NCAI guidelines)
- Informed consent/assent protocols
- Data security (encrypted storage, limited access)

**Data Sovereignty**:
- Community Research Teams control data from their sites
- Tribal communities retain ownership of Indigenous knowledge
- Data governance agreements specify use, access, dissemination

**Equity in Research Process**:
- Compensation for community members' intellectual labor
- Participatory decision-making (not extractive research)
- Capacity building for CRTs
- Co-authorship opportunities for community members

**Conflicts of Interest**:
- No financial relationships with EdTech vendors
- Transparency in reporting (including null findings)
- Independent external evaluation

---

**Word Count**: 4,138 (Target: 3,500-4,500 for Methodology and Timeline section)
